{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./pkg/db.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, words, wordnet\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "conn_string = os.getenv('POSTGRES_CONNECTION_STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_dataframe('lyrics_limit.sql', conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "df_stop = df[~df['word'].isin(stops)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing non-english words\n",
    "eng_words = set(wordnet.words())\n",
    "eng_words_stem = set([stem(word) for word in eng_words])\n",
    "df_stop_eng = df_stop[df_stop['word'].isin(eng_words_stem)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_eng.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the list of documents for count vectorizer\n",
    "doc_list = []\n",
    "grouped = df_stop_eng.groupby('track_id')\n",
    "for doc_id, group in grouped:\n",
    "    word_counts = []\n",
    "    for index, row in group.iterrows():\n",
    "        word_counts += [row['word']] * row['count']\n",
    "    doc_str = ' '.join(word_counts)\n",
    "    doc_list.append(doc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert document list to count matrix\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply NMF to the matrix of word counts\n",
    "num_topics = 10\n",
    "model = NMF(n_components=num_topics, init='nndsvd')\n",
    "model.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 10 words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    print('Topic #%d:' % topic_idx)\n",
    "    print(' '.join([feature_names[i] for i in topic.argsort()[:-11:-1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3de90a5f257a90c7d524da2e947aa75056eaab7c9b8a3ec9d793f7764de89c94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
